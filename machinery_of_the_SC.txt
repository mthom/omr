DISCLAIMER: This is a very rough set of notes on the implementation of the
shared cache class. Our ambition is to transform it into a detailed, technically well-founded
white paper on the shared cache class, which (presumably) will be released for the
benefit of the general public (if not, at least internally to IBM and CASA). For now,
this document must remain INTERNAL TO CASA.

--

First, it's instructive to look at the J9PORT_SHR_CACHE_TYPE enum. It's not an enum..
they are defined separately as individual constants/macros, defined as 1-5. Their 
names are:

*_PERSISTENT
*_NONPERSISTENT
*_VMEM
*_CROSSGUEST
*_SNAPSHOT

I don't know what distinguishes the last of these two. Or _VMEM.  J9_SHR_CACHELET_SUPPORT
is linked to OsCachevm.hpp, so I suppose cachelets are connected to whatever the VMEM
cache type handles.

Starting with the data and functionality of OSCache.hpp and sattelite modules.

The inheritance diagram for the OSCache*.[c|h]pp files is:

			OSCache
				|
			OSCacheFile
			  /    \     \
			/       \      -------\
	OSCachemmap		OSCachevmem  OSCachesysv
	
These are all preceded by the characters "SH_" within C++, BTW.
	
Certain crucial methods in OSCacheFile (like the very instructive createCacheFile
method) are non-virtual. Indicating that OSCachemmap and OSCachevmem instantiate
their parent objects with different mode creation settings, the mechanics of 
which are otherwise the same.

Note that SH_OSCacheInitializer is provided for use by consumers of the OSCache
family. They tell the OSCache* class how to initialize data in the class. Or,
the class itself. I suppose! I don't really know the direct purpose of it. To
initialize the class at key moments, yes? I don't think OSCache presumes much
about the client.

OSCACHE.HPP MEMBER VARIABLES:

As the header comment says.. the purpose of the class is to abstract over the 
shared memory region that the class will be mapped into, and it also contains mutexes
for read/write access.

We run down the list.

	char* _cacheName; // the name of the cache. command line argument.
	/* Align the U_64 so we don't have too many platform specific structure padding
	 * issues in the debug extensions. Note C++ adds a pointer variable at the start
	 * of the structure so putting it second should align it.
	 */
	U_64 _runtimeFlags; // not sure what values these might assume.
	U_32 _cacheSize; // size of the cache in bytes.
	void* _headerStart; // where does the cache header start?
	void* _dataStart; // and its data?
	U_32 _dataLength; // how large is the data segment?
	char* _cacheNameWithVGen; // cache name with version and generation included.
	char* _cachePathName; // its filesystem path.
	UDATA _activeGeneration; // which is the currently active generation?
	UDATA _createFlags; // configures the creation of a cache (?)
	UDATA _verboseFlags; // verbosity options.
	IDATA _errorCode; // the active error code? possibly an OK code?
	const J9SharedClassPreinitConfig* _config; // 
	I_32 _openMode; // passed to j9file_open or j9file_blocking_async_open
					// (platform dependent)
	bool _runningReadOnly; // cannot write? wouldn't it be handled by _openMode?
	J9PortLibrary* _portLibrary; // the port library. platform dependent.
	char* _cacheDirName; // The parent directory of the cache.
	bool _startupCompleted; // was startup completed???
	bool _doCheckBuildID; // dunno.
	IDATA _corruptionCode; // did some sort of corruption take place?
	UDATA _corruptValue; // Which value is known to be corrupt?
	bool _isUserSpecifiedCacheDir; // used the default cache dir?
	
Surely you notice the lack of mutexes. The member functions SH_OSCache contains that
have to do with acquiring locks are pure virtual functions. ACKSHAULLY.. these methods..
acquiring and releasing locks and so forth.. are defined in OSCacheFile, which is at
the root of both OSCachevmem and OSCachemmap.

Interesting comment from Hang Shao (dated Jun 19 of this year):

"OSCachevmem.hpp/OSCachevmem.cpp is not being used now. They are only active under 
J9SHR_CACHELET_SUPPORT, which is not defined.

We could either remove these two files together with all the code inside 
J9SHR_CACHELET_SUPPORT,
or simply add vm parameter and put #include "OSCachevmem.hpp" inside 
ifdef J9SHR_CACHELET_SUPPORT here."

Fun!! So it isn't necessary at all. I don't understand what cachelets are.
OSCACHE.HPP MEMBER FUNCTIONS:

There is a 'newInstance' method, so it is a singleton class... well, no. That's what
I initially thought. Instead, it's a helper method that creates a SH_OSCache according
to the versionData->cacheType attribute. If it's of _VMEM type, then we construct
a vmem object, etc. Interestingly, _VMEM object construction is supported only if
J9SHR_CACHELET_SUPPORT is on, which per the earlier comment is never defined! So I don't
think we have to worry about it at all, really. _SYSV is the version used if a
nonpersistent cache is chosen, BTW.. doesn't appear particular to the system architecture
used at all!! Contra our earlier impressions.

One thing to note is that OSCachevmem uses j9file_read and j9file_blocking_async_read,
while OSCachemmap does not. My impression is that 'cachelets' are smaller versions
of the cache that are copied into main memory, though from a file (and hence they
are persistent!). Does that mean they're written back to file?

Despite the annotation, j9file_read is a macro defined in runtime/oti/j9port_generated.
It expands to a call to the port library to call whatever file reading routine the 
host platform provides. A great many other system calls are wrapped in macros and
named similarly within the file. 

See runtime/port/sysvipc for a collection of wrappers around System V style ipc primitives.
Mostly for shared memory and semaphores, although there is j9sharedwrapper.[h|c], which
provides some helper routines for dealing with the shared class cache. You see, strewn
throughout, references to a 'control file', for shared memory..

This is what the whole ctrlDir convention has been about!! Setting up the control file
directory. From the OpenJ9 manual:

"Nonpersistent caches are stored in shared memory and have control files that describe 
the location of the memory. Control files are stored in a javasharedresources subdirectory 
of the cacheDir specified. Do not move or delete control files in this directory. The 
listAllCaches utility, the destroyAll utility, and the expire suboption work only in the 
scope of a given cacheDir."

Nonpersistent caches are still shared!! You must keep this in mind.

Going back to OSCache, though, it appears to be mostly utilities for extracting
the Java version and cache generation info from cache filenames / control files,
depending on which is used (ie. is the cache persistent or nonpersistent?). 
The only really non-trivial parts are those dealing with initialization and
startup. 

'commonStartup', as the name suggests, deals with the initializing the portions
OSCache manages that are common to all its variants: vmem, Sysv, and mmap. Many of the
utilities are statically initialized. commonStartup, again, is mostly concerned with
initializing everything properly.. the cache names, their version and generation flags,
various runtime and verbosity flags.. administrivia! 

'commonInit' sets the OsCache data to 0 or NULL.  Or FALSE, if the values are boolean.
This exhaustively describes what it does. See for yourself.

'commonCleanup' is similar.. it mostly calls j9mem_free_memory on the string data that
were allocated in commonStartup, and it emits some trace messages as it does so.
Nothing interesting or spectacular.

'initOSCache' does exactly that. It writes the _dataLength and _dataStart attributes
into the header, along with the version, generation and build ID (a SHA hash) to the
header.  Notice that OSCache does nothing at all with _dataStart and _dataLength, beyond 
initializing them and writing them to the cache header. These belong to the subclasses!

'checkOSCacheHeader' simply verifies that several important invariants hold.. ie.
the size of the cache is equal to the size of the cache header + _dataLength.

Then there's some functions for specifying the corruption context. Which is just a code,
and the value that points to the corrupted thing or provides some helpful data, usually.
It's all in the comments, no need to reiterate that here.

Moving onto OSCacheFile.hpp !

It's another abstract class. It doesn't initialize the 'initialize' virtual function
or anything. It wraps a single file handle as its lone data member.

It also defines some OSCache_mmap_header structs, which contain (among other things)
locks for the header and data sections. OSCache_mmap_header_1 and *_2 contain many
of the same members but are slightly different. *_1 appears to contain some 'unused'
padding that's probably used to make them the same size. Hmmm.. yeah, I dunno.
Actually appears to have more to do with OSCache_header1 versus OSCache_header2. 
The mmap_header_*'s wrap one of each. Going by the #define's in OSCache.hpp, they
appear to correspond to different cache generations.

OSCACHEFILE member variables:

	IDATA _fileHandle;
	
And that's it!! That's all the class wraps. It's a handle for a file. That is it.

OSCACHEFILE member functions:

'acquireHeaderWriteLock' reaches into the header, and grabs the header write lock,
calling out to the j9file_lock/j9_file_async_blocking_lock primitives to acquire it.
With traces and error handling, as always.

'getmmapHeaderFieldOffsetForGen' finds the byte offset of a given field (second argument)
with the OSCache header, depending on the generation passed to it (first argument). 

'releaseHeaderWriteLock'.. similar, analogous thing. Clear the lock. 

'tryAcquireAttachWriteLock' does.. pretty much the same as 'acquireHeaderWriteLock',
only it gets the attach write lock.. to write to the attach region of the cache.
I don't know what the 'attach region' is, exactly. I suppose we'll find out!
But the logic is basically the same. I don't why the function name begins with 'try'
and not 'acquire'.. there's nothing exceptional about it. No, I see the difference
now. One of the flags has a NOWAIT stipulation. If the lock is held, it will immediately
return to the caller.

'checkCacheHeaderValid' checks that the cache header is well formed and forms
a corruption context if it's not, and yadda yadda. Most of this stuff is straightforward
and uninteresting!! Mostly has to do with acquiring and releasing locks. It assumes
the cache is persistent, since, y'know, a cache formed at runtime must be valid, yes??
There's no chance it was violated by openj9. I expect this to be the assumption.

`openCacheFile' opens the cache attached to the file header. Nothing interesting
about it. Uses the _openMode flags and such. Blah.

'findfirst', 'findnext', and 'findclose' iterate through cache files in the cacheDir
(a parameter for all three functions). There's a helper predicate that determines
whether the files found are valid cache name files, and if they are, it returns the
file handle of those files.

And then we have two other functions checking cache access permissions attached
to the file handle. These are static functions of OSCacheFile.

And we're done! Easy.

OSCACHESYSV

Ok, in OSCachesysv.hpp, we have a few different header types corresponding to the 
various cache generations. Again, they are similar to the mmap headers we saw
last time (defined in OSCacheFile.hpp, confusingly). This time, they contain things
like semaphore IDS (but never shared memory segment IDs).

REMEMBER, OSCachesysv is NOT inherited from OSCacheFile!! It's a shared memory region
after all, not a cache.

OSCachesysv member variables:

	j9shmem_handle* _shmhandle;
	j9shsem_handle* _semhandle;

	IDATA _attach_count; // # of processes attached to the region.
	UDATA _totalNumSems; // total number of semaphores active.
	UDATA _userSemCntr;  // the number of semaphores requested by users.
	U_32 _actualCacheSize; // the actual size of the cache in bytes.

	char* _shmFileName; // initialized as 'cacheNameWithVGen' in startup
	char* _semFileName; // not sure!
	bool _openSharedMemory; // did we fail at attempting to open an existing shared memory??
	
	UDATA _storageKeyTesting;

	const J9SharedClassPreinitConfig* config;

	SH_OSCache::SH_OSCacheInitializer* _initializer;
	UDATA _groupPerm; // group permissions.

	I_32 _semid; // ID of the seamphore.

	SH_SysvSemAccess _semAccess;
	SH_SysvShmAccess _shmAccess;

	J9ControlFileStatus _controlFileStatus; // the status of the shared memory control file.
	
Clearly, these are much more elaborate than the other ones.	

OSCACHESYSV MEMBER FUNCTIONS:
\
`startup` is a few hundred lines long, and mostly deals with the myriad cases involved
in trying to create a new shared memory, or attach to an existing one.

`openCache` is called if the caller holds the mutex... but it says as much. First,
a few helper functions!!

`OpenSysVMemoryHelper` does the thing of calling out to j9shmem_open and related
functions. It even calls out to 'Deprecated' versions of j9shmem_open if the generation
is less than current. So that is good.

'shmemOpenWrapper' calls to OpenSysVMemoryHelper, and tries to open again if the previous
read failed if an option was set to attempting a read-only open upon initial failure.

'openCache'.. calls shmemOpenWrapper and branches on the result code. the *_FAILED cases
consist mostly of printing an error message somewhere and promptly returning. For the
opening (of an existing) shared cache case: check that there is a user specified
cacheDir, and also get the shared memory access settings by calling 
'checkSharedMemoryAccess'. If all checks out, do nothing. If we created the cache by
opening it, call 'initializeHeader'. Otherwise, usually.. an error happened, and
we simply print it out.

`verifyCacheHeader`.. checks the validity of the cache header and assorted nonsense. Yeah.
Sets the corruption context and/or prints error messages where applicable.

'detachRegion' detaches from the _shmHandle memory region, and files error info if
that doesn't succeed.

'cleanup' calls 'detachRegion', and closes _shmHandle, using j9shmem_close. Similarly
for _semhandle.. if they are both non-NULL. 'commonCleanup' is called.

'detach' ... detaches the region. But adds some trace messages on top of calling 
'detachRegion'.

'initializeHeader' checks that the cacheSize and other features are set up appropriately,
before attaching to the region by calling j9shmem_attach. If this doesn't succeed,
ErrorInfo is recorded and we return. Otherwise, _headerStart, _dataStart and _dataLength
are all written to (dataLength was determined previously in the function). We copy
the shared memory eyecatcher into the header, call 'initOSCacheHeader', and importantly,
we call the _initializer's init virtual function, if _initializer is non-NULL. Then
we toggle _cacheInitComplete as true.

'attach' similarly attaches to an existing region if one exists at _shmHandle.
From there, it calls 'verifyCacheHeader', and if the cache header is found to be invalid
or corrupt, it returns with an error after detaching _shmHandle. Otherwise, it initializes
_dataStart and _dataLength as 'initializeHeader' does, and increments the _attachCount.
Curiously, it doesn't seem to bother with mutexes.. I suppose because a header, once
initialized, is a read-only structure. I don't think _attachCount is atomic either..
yeah, it's not. It's just an old fashioned integer.

'destroy' starts by calling 'DestroySysVShmHelper', which extracts the version and generation
from the cacheName, and uses them to decide which j9shmem_destroy[Deprecated]? routine
to call. Very much in parallel with OpenSysVHelper. Before this, it detaches from the
shared memory region, and checks that the cache isn't active (ie. the number of processes
attached isn't greater than 0). Then we call 'DestroySysVSemHelper' and so forth.

'getNewWriteLockID' returns a new, unoccupied ID, from its store of unused write lock IDs.
If there are none left, it returns -1.

'acquireWriteID' awaits on a passed write lock id, and returns it when j9shmem_wait_deprecated
returns, if successful. Otherwise it prints an error message and returns.

'enterHeaderMutex' and 'exitHeaderMutex' acquire the specially designated SEM_HEADERLOCK
as the previous two functions do, not much difference in how they work.

'isCacheActive' retrieves the status of the shared memory region using j9shmem_stat,
and checks that the number of processes attached is > 0. If so, return true; otherwise,
false.

'printErrorMessage' translates J9PORT_ERROR_SYSV_IPC_* values to trace messages.

'cleanupSysVResources' tries to detach and destroy both _shmHandle and _semHandle.
If 'isCacheActive' returns true, it simply detaches and closes the handles. There's 
extensive error handling and platform specific code laden throughout.

'errorHandler' prints the passed error message and calls 'cleanupSysVResources' if
startup didn't complete, and we meant to create shared memory resources.

'setRegionPermissions' is the barest of wrappers around j9shmem_protect.

Same for 'getPermissionsRegionGranularity'.

Most of the remaining functions are quite straightforward. What I'm drawn to primarily
is 'getControlFilePerm', which gets the permissions of the control file, mostly..
interesting that the name of the control file is stored outside the OSCachesysv object.
Actually, no, the name is either 'memory' or 'semaphore'. Huh.

'restoreSnapshotFile' is a tour de force through OSCacheSysv's internal functionality,
and through SH_CacheMap's as well! I think that about sums up its methods.

OSCACHEVMEM

That's right! It's quite small, so I think we can devote a quick aside to it. Here are
its member variables:

	I_64 _actualFileLength;
	UDATA _writeLockCounter;
	
	omrthread_monitor_t _lockMutex[J9SH_OSCACHE_VMEM_LOCK_COUNT];

That's the list. We have mutexes and a counter for write lock. Also, OSCachevmem
inherits from OSCacheFile, so it does have a file handle. Interesting.

Yeah, pouring over this, looks to be very very much like an mmap! That's the 
header format it uses.. all the rest is just logic for 'attaching' to a file, and
setting up the internals of the object.

OSCACHEMMAP

Like OSCacheVmem, it also inherits from OSCacheFile. 

Member variables:
	I_64 _actualFileLength;
	J9MmapHandle *_mapFileHandle;
	
	UDATA _finalised;
	
	omrthread_monitor_t _lockMutex[J9SH_OSCACHE_MMAP_LOCK_COUNT];
	
	SH_CacheFileAccess _cacheFileAccess;
	
That's all she wrote! Notice the addition of the _mapFileHandle clause thingum.
Something OSCacheVmem didn't have. I guess the virtual memory versus was.. a lightweight
spin on this more elaborate idea? Maybe a paring down of functionality, or management
responsibility?

Looking at the startup method, I see that OSCacheSysv's startup is largely mirrored,
in terms of what is checked.. only here we fall back on the 'openCacheFile' of OSCacheFile.

It's 'internalAttach' which calls j9mmap_map_file, and passes the _fileHandle field
along to it. It returns the _mapFileHandle. Then some error checking is done, the 
header is gotten at eventually, the _dataStart and _dataLength things are loaded into
it.. pretty unremarkable stuff I'd say!!

ok, moving on from the OSCACHE* files...

.. and to the CompositeCache classes.

SH_CompositeCache is a pure abstract class defining a minimal interface:

class SH_CompositeCache
{
public:
	virtual U_16 getJVMID(void) = 0;
	virtual bool isRunningReadOnly(void) = 0;
	virtual UDATA getTotalUsableCacheSize(void) = 0;
	virtual void getMinMaxBytes(U_32 *softmx, I_32 *minAOT, I_32 *maxAOT, I_32 *minJIT, 
								I_32 *maxJIT) = 0;
	virtual UDATA getFreeAvailableBytes(void) = 0;
	virtual U_32 getDebugBytes(void) = 0;
	virtual void setInternCacheHeaderFields(J9SRP** sharedTail, J9SRP** sharedHead, 
								U_32** totalSharedNodes, U_32** totalSharedWeight) = 0;
	virtual bool isStarted(void) = 0;
	virtual IDATA restoreFromSnapshot(J9JavaVM* vm, const char* cacheName, bool* cacheExist) = 0;
	virtual I_32 tryAdjustMinMaxSizes(J9VMThread *currentThread, bool isJCLCall = false) = 0;
};

SH_CompositeCacheImpl is much much larger. The implementation of the CompositeCacheImpl
class, which inherits from SH_CompositeCache, of course, is in CompositeCache.cpp.

Composite caches are linked lists of composite caches that represent the view of shared
class in terms of blocks of memory. Each node in the list has its own locks for reading
and writing. Cache entries grow from the back, memory segments from the front. 
CompositeCache.cpp contains a helpful diagram. Top of the file, can't miss it!!

The start of the block (the node in the list managed by this object) is stored in
the member variable _theca.

A J9SharedCacheHeader contains a bunch of useful information about the layout of each
cache block.. its updateSRP, readWriteSRP, etc.. how they're all maintained in relation
to one another. Also, it has lots of statistics on how many bytes are taken up by
different types of data, like JIT and AOT code.

Importantly, each SH_CompositeCacheImpl contains an SH_OSCache *. Part of our work will
be uncovering the relationship between the two!

BTW, a cachelet is an SH_CompositeCache nested *within* an SH_CompositeCache!! Why you'd
ever want to use it (and from Hang Shao's comments, they do not!) I dunno. 
'newInstanceNested' and 'newInstanceChained' are both walled off behind the 
J9SHR_CACHELET_SUPPORT conditional, so we will not document them here (for now).

Back to _theca.. it's not set until way down in ::startup. Late in the file.

In 'startup', cacheMemory is argument that is non-NULL only if we are UnitTesting
the cache and want to avoid using the OSCache, per the comments. Otherwise, we are
using the OSCache to store shit in. _oscache is initialized primarily in 'initialize',
but a few other places as well. The cacheName is provided to SH_OSCache's static
'newInstance' function, which builds the subclass of SH_OSCache that applies, as we
saw earlier.

Back to 'startup'. oscache calls its 'startup' method, and we pass _runtimeFlags,
_verboseFlags, etc. along to it. Then, if we 'hasWriteMutex' (ie. the CompositeCache
object's 'startup' secured the CompositeCache's write mutex), we do something like this:

		if (cacheMemory != NULL) {
			_theca = (J9SharedCacheHeader*)cacheMemory;
		} else {
	    	_theca = (J9SharedCacheHeader*)_oscache->attach(currentThread, &versionData);

That's right, we get the _dataStart of _oscache and store it in our _theca. Yep.
Then we check if the cache hasn't already been corrupted. Presumably by a concurrently
running VM that is trying to do the same thing we are. If so, do the usual error reporting
song and dance and return; if not, do a veritable fuckton of other crap. Corruption
and error checking and reporting, and writing numbers to stats variables, mostly.

_theca is a J9SharedCacheHeader, which looks like this:

typedef struct J9SharedCacheHeader {
	U_32 totalBytes;
	U_32 readWriteBytes;
	UDATA updateSRP;
	UDATA readWriteSRP;
	UDATA segmentSRP;
	UDATA updateCount;
	J9WSRP updateCountPtr;
	volatile UDATA readerCount;
	UDATA unused2;
	UDATA writeHash;
	UDATA unused3;
	UDATA unused4;
	UDATA crashCntr;
	UDATA aotBytes;
	UDATA jitBytes;
	U_16 vmCntr;
	U_8 corruptFlag;
	U_8 roundedPagesFlag;
	I_32 minAOT; // minimum reserved space for AOT code.
	I_32 maxAOT;
	U_32 locked;
	J9WSRP lockedPtr;
	J9WSRP corruptFlagPtr;
	J9SRP sharedStringHead;
	J9SRP sharedStringTail;
	J9SRP unused1;
	U_32 totalSharedStringNodes;
	U_32 totalSharedStringWeight;
	U_32 readWriteFlags;
	UDATA readWriteCrashCntr;
	UDATA readWriteRebuildCntr;
	UDATA osPageSize;
	UDATA ccInitComplete;
	UDATA crcValid;
	UDATA crcValue;
	UDATA containsCachelets;
	UDATA cacheFullFlags;
	UDATA readWriteVerifyCntr;
	UDATA extraFlags;
	UDATA debugRegionSize;
	UDATA lineNumberTableNextSRP;
	UDATA localVariableTableNextSRP;
	I_32 minJIT; // minimum reserved space for JIT.
	I_32 maxJIT;
	IDATA sharedInternTableBytes;
	IDATA corruptionCode;
	UDATA corruptValue;
	UDATA lastMetadataType;
	UDATA writerCount;
	UDATA unused5;
	UDATA unused6;
	U_32 softMaxBytes;
	UDATA unused8;
	UDATA unused9;
	UDATA unused10;
} J9SharedCacheHeader;

'getRequiredConstrBytes' indicates that is an OSCache instance is to
be built with the object (ie. we are not doing any UnitTest'ing),
since the sizeof(OSCache) is included in the reqBytes value returned
by the function!! This indicates they are to be built on the same
block.

'getRequiredConstrBytesWithCommonInfo' gets the constructor size of
'getRequiredConstrBytes', plus the size of
J9ShrCompositeCacheCommonInfo, which houses data common to *all*
composite caches, as the name would suggest. It's:

typedef struct J9ShrCompositeCacheCommonInfo {
	omrthread_tls_key_t writeMutexEntryCount;
	struct J9VMThread* hasWriteMutexThread;
	struct J9VMThread* hasReadWriteMutexThread;
	struct J9VMThread* hasRefreshMutexThread;
	struct J9VMThread* hasRWMutexThreadMprotectAll;
	U_16 vmID;
	U_32 writeMutexID;
	U_32 readWriteAreaMutexID;
	UDATA cacheIsCorrupt;
	UDATA stringTableStarted;
	UDATA oldWriterCount;
} J9ShrCompositeCacheCommonInfo;

ShcItemHdr is this:

typedef struct ShcItemHdr {
	U_32 itemLen; 		/* lower bit set means item is stale */
} ShcItemHdr;


Why do I bring this up?? Because the composite cache can be
'scanned'.. that is, its contents can be walked.

I went down to 'nextEntry' to see how the cache is iterated over,
starting from 'findStart'. It depends on these macros, defined in
shcdatatypes.h :

#define CCITEMLEN(ih) (J9SHR_READMEM((ih)->itemLen) & 0xFFFFFFFE)
#define CCITEMSTALE(ih) (J9SHR_READMEM((ih)->itemLen) & 0x1)
#define CCSETITEMLEN(ih, len) ((ih)->itemLen = (len & 0x1) ? len+1 : len)
#define CCSETITEMSTALE(ih) ((ih)->itemLen |= 0x1)
#define CCITEM(ih) (((U_8*)(ih)) - (CCITEMLEN(ih) - sizeof(ShcItemHdr)))
#define CCITEMNEXT(ih) ((ShcItemHdr*)(CCITEM(ih) - sizeof(ShcItemHdr)))

'nextEntry' skips over stale items within the cache (which it counts,
if staleItems is a non-NULL pointer). The ih values (item headers) are
given by 'next'. 'nextEntry' returns the first non-stale item it
can find, if it can found one.

'next' sets BlockPtr free to UPDATEPTR(_theca), which has this definition:

#define UPDATEPTR(ca) (((BlockPtr)(ca)) + (ca)->updateSRP)

We need the refresh mutex *or* the write mutex! I don't know what the
difference is between them, but there is an Assert that checks that
one of them is held.

ih points to _scan, which starts when the object is initialized as
NULL. That's in 'commonInit.' When 'startup' is called, _scan
is initialized as

   _prevScan = _scan = (ShcItemHdr*)CCFIRSTENTRY(_theca); // line
   1463.

'findStart' initializes _scan as:

_prevScan = _scan;
_scan = (ShcItemHdr*)CCFIRSTENTRY(_theca);

Same basic thing.

CCFIRSTENTRY has this definition:

#define CCFIRSTENTRY(ca) (((BlockPtr)(ca)) + (ca)->totalBytes -
 (ca)->debugRegionSize - sizeof(ShcItemHdr))

In each case, it goes to the start of the cache entry lineup, and
skips over the header. Seems that.. the aim is to look past _scan for
the next cache entry in the sequence, while also checking conditions
that indicate the cache is corrupted.

UPDATEPTR(_theca) is the allocation pointer. The point at which
new cache entries are allocated. As evidence of this, witness!!!
Line 2952:

BlockPtr allocPtr = UPDATEPTR(_theca);

Context! fillCacheIfNearlyFill. The description: fill the cache with
dummy data for some reason if there's not enough free space left
beyond what's allocated for AOT and JIT code. Fill the remainder with
stuff that's not NULL. No idea why, as yet.

Anyway, yes.. free is the allocation pointer. A bump pointer into
the cache. If ih exceeds free, then it points at an item. This is
clear. It returns the next available item, at *ih, as its result
if no corruption is found. 'next' is ONLY called from 'nextEntry',
according to the comment.

/*
 * This method fills up cache with dummy data.
 * Dummy data added is a sequence of 0xD9 byte.
 * Cache is filled only if (available bytes - (AOT + JIT reserved bytes)) < CC_MIN_SPACE_BEFORE_CACHE_FULL.
 * Dummy data is added such that a gap of J9SHR_MIN_GAP_BEFORE_METADATA bytes is maintained between classes and metadata.
 * After adding the dummy data, it sets the runtime flags to mark the cache full.
 *
 * Note: This method is not called for realtime cache. If ever it needs to be called for realtime cache,
 * it would have to be modified to call setRuntimeCacheFullFlags() on first super cache.
 * ...
 */

'reset' calls findStart(currentThread) to set the scan pointers back
to UPDATEPTR(_theca). And _stored* statistics back to 0, besides. It
also unlocks the cache. Should the cache be locked before hand? Hard
to tell.

Back to the initialization dance.

'setCacheAreaBoundaries' sets up the various boundaries of the cache!!
According to the diagram that leads the source file, I
suppose. Apparently, an SRP hashtable is also stored within its
depths!! This determines the value of finalReadWriteSize, in part,
under the right conditions. It contains "numOfSharedNodes"
entries. I'm not sure what a "shared node" is.

It goes through some trouble to determine finalReadWriteSize.  This
determines the size of the read/write area after the shared cache
header. The start of the segment area is much much easier to compute.
If the "ENABLE_ROUND_TO_PAGE_SIZE" option is enabled, then
finalSegmentStart, newCacheEnd, etc. are rounded to the size of a
page. Whatever the value of _osPageSize is.

"Currently the read write area of composite cache is only used by
shared string intern table."

Sounds as though that's the SRP hash table that determines the size of
the read/write area! Then, since we're not using cachelets (we never
use cachelets!), the ClassDebugDataProvider calls its static
'HeaderInit' method.

A debug message is printed if the composite cache size was rounded
down to a page's size, and we're done.

But verily, we are writing segmentSRP and updateSRP in this
segment. So it's all good, I think.

'initBlockData' initializes an ShcItem type. Which contains a type
tag, length, and Java VM ID. That's it.

'enterReadMutex' increments the reader count (by calling
'incReaderCount') and waits for the cache to be unlocked, if it's
currently locked. It will block until the cache is locked. Or the wait
count is exceeded. If it is locked, the reader count is immediately
decremented, and 'acquireWriteLockMutex' is called. I'm not sure of
the scope of the read/write mutexes here.. I suppose because they're
object methods, they govern their parent objects. If the write lock
mutex was acquired successfully, 'incReaderCount' is called *again*.
Then the write lock is released, by calling 'releaseWriteLock'.

'incReaderCount' uses VM_AtomicSupport to do a 'lockCompareExchange'
on the reader count, which may fail! Suggesting that high-granularity
locks are being used, rather than, you know, actual atomics. Something
belied by the name. These calls to 'lockCompareExchange' are
buttressed by a call to 'protectHeaderReadWriteArea' and
'unprotectHeaderReadWriteArea'.

'protectHeaderReadWriteArea' protects the header of the read/write
area? If we're protecting the header, or we're protecting the read
write area and also changing it,

_readWriteAreaPageStart is the rounded down size of the read/write
area (rounded down to a page size, of course). I think the header,
read/write area, and cache area all occupy a single page? I think??

/* Calculate the page boundaries for the CC header and readWrite areas, used for page protection.	
 * Note that on platforms with 1MB page boundaries, the header and readWrite areas will all be in one page */

ok, that's pretty damn unclear. It's just rounded it *to* a multiple
of the page size, in terms of address, yes? so that the boundaries
align??

It's possible for _readWriteAreaPageStart and _cacheHeaderPageStart to
coincide as integers. This makes a bit more sense with all the
rounding, then. If not, there is this comment:

/* Header is smaller than a page, round up the start of the readWrite area to the next page
 * to ensure that it can be protected/unprotected independently */

Seems they can be on either a single page or two separate pages.

Anyway, I get the areaStart.. based on whether the caller wants to
protect the header, the read/write area, or both.. it calls
'setRegionPermissions' to set the page permissions if areaStart is
non-NULL. Then it decrements the [_readWrite|_headerProtect]Cntr
variables. And then the _commonCCInfo->hasReadWriteMutexThread is set
to NULL, away from the current thread, and similarly for
_commonCCInfo->hasRwMutexThreadMprotectAll. Presuming that
currentThread == the latter.

'setRegionPermissions' checks that length is positive, and that the
OSCache _oscache exists, before deferring to
_oscache->setRegionPermissions. If _oscache is NULL, j9mmap_protect
is called instead.. but why might _oscache be NULL? Is it known to the
head composite cache only? Why then is a new instance created at
startup? Perhaps 'startup' deters that other CompositeCache's exist,
at which point it dredges up the parent's OSCache.

'unprotectHeaderReadWriteArea': same thing. has logic to do nothing
if some other thread is changing the two (or one, where applicable).
Call 'setRegionPermissions' to unprotect.

'startupForStats' is the method to call to create a CompositeCache
for a shared cache that exists, and is attached already. An 'oscache'
parameter is passed into it. I don't think its comment has been
updated in some time, it definitely sounds.. behind. But it makes it
sound as though oscache is the 'attached shared memory region'. Or its
representative as an object, anyway.

It sets _oscache to oscache, one of its arguments, to answer our
earlier question. attachedMemory, mentioned in the now obsolete
comments above the definition of the method, is now derived from
this line:

attachedMemory = (BlockPtr)oscache->getAttachedMemory();

It configures the CompositeCache object by extracting information from
oscache.

'allocateBlock' allocates a block of memory. Its parameter itemToWrite
is a ShcItem, which serves as a header for the block.. containing its
type, length, and Java VM ID, as we saw previously.

'allocate', which 'allocateBlock' and friends call, is not static,
which is odd when you consider its parameter names. segmentBuffer,
and readWriteBuffer, especially. 'allocateBlock' passes in NULL
values for these.

The rest of the function that isn't immediately straightforward is
a dispatch on the value of type, as a series of if/else if blocks.

if allocating aot:

... it gets the available AOT bytes!! Naturally. The ones that were
reserved. This is the task of 'getAvailableReservedAOTBytes'.

'getFreeAOTBytes' returns the AOT bytes that are reserved for AOT
allocations, but that have not yet been allocated. Regardless of how
many of the reserved AOT bytes are available, we flag the
regionFullFlag bit vector, toggling J9SHR_AOT_SPACE_FULL (!)..
to indicate that other threads should back off on AOT allocations
until we're finished, I guess? What could be the reason for that???

The JIT case allocation is almost identical to the AOT one!!
Substitute 'JIT' for 'AOT' in the code and you're most of the way
there! We also mark the regionFullFlag with J9SHR_JIT_SPACE_FULL!

If readWriteBuffer hasn't been decided yet, we compute a boolean value
called enoughSpace, which is self-descriptive.

It moves onto checking that 'enoughAvailableSpace' is true, which is
so if enoughSpace is true, and usedBytes + usedBytesInc <=
softMaxValue. The soft max, not the hard max! Unsure of the
distinction.

Any allocate type that isn't AOT or JIT counts as metadata! According
to the comments within the code.

So, we call 'allocateMetadataEntry'. The past two paragraphs are
applicable if itemLen > 0. We then check to see that
separateBufferSize > 0.

If we yes, and segmentBuffer != NULL, the point the segmentBuffer
pointer at SEGMENTUPDATEPTR. The END of the segment section,
where new segments are added. We need to look
'changePartialPageProtection', which is the thing that is allocated
next.

If readWriteBuffer is allocated, we call allocateReadWrite.. which
allocates space in the read/write area, I can only guess. When was
readWriteBuffer sent to something non-NULL? Answer: when it was passed
in, by the caller. If cachelets and crap aren't enabled, we use
RWUPDATEPTR. Obviously.

If there is 'enoughSpace', but not 'enoughAvailableSpace', the cache
space is enough, but available space is not! So we defer to either the
head of the cache linked list, or the parent's cache, if either exist,
checking in that order.

If we mean to allocate a block, we call increaseUnstoredBytes, with
usedBytesInc as the amount of space needed, I presume.

Otherwise, we use the common cache info struct to set cache header
full flags, using the value of the regionFullFlag. There's our
answer.. the regionFullFlag, true to the name (the lack of an opening
underscore), is just a local variable whose value impinges on nothing
until this moment.

Otherwise! enoughSpace is false, our life is a lie! We go about
markings flags with the bits signified by
J9SPACE_[JIT|AOT]_SPACE_FULL, as before! But we don't have
regionFullFlag at our disposal anymore. Some other logic applies if we
are nested (ie. cachelets are enabled), but we're not considering it
just yet.

'allocateMetadataEntry' .. seems to allocate a cache entry, not a
segment but an actual, no shit entry, against the cache! And does the
actual work of writing stuff into the cache that we never saw directly
in 'allocate'.

It begins by carving out space, from the allocPtr, for a ShcItemPtr
(it moves backwards to do this, seeing as how it's a cache entry).
Then it invokes partial page protection at the allocPtr, using
'changePartialPageProjection' (it passes the address). Then good old
fashioned memcpy is called!! I have no idea how this is or isn't
affected by the partial page protection scheme. But I supposed I'll
find out. Oh, and the previous _scan and _prevScan values are backed
up to _storedScan and _storedPrevScan respectively, so that _prevScan
is set to scan, and _scan refers to the item after the item header.
Then the result is returned.

'changePartialPageProtection' does some rounding of the passed in
address according to the page alignment, and calls
'setRegionPermissions' on that address. If the address passed in is
already page aligned, we jump to the done label, which does nothing
but emit a trace message.

'allocateWithSegment' calls allocate, asking for a block to be
allocated as the allocation type.

'allocateJIT' calls allocate, asking for a JIT type.

'allocateAOT' calls allocate, asking for an AOT type.

'decReaderCount' does the decrement/wait on 'atomic' lock and exchange
loop again. This time to decrement _theca->readerCount. Once finished,
it calls 'protectHeaderReadWriteArea'. Not a typo.

'enterWriteMutex' either acquires the write lock through the owning
osCache (either or our own if _ccHead or NULL, or the head's oscache,
lending further creedence to the idea that there is only *one* OSCache
for every chain of CompositeCache's, of which there is also just one),
or by using omrthread_monitor_enter. Then a bunch of other options
that might revoke possession of the mutex if toggled are
checked. That's basically it, not much more to say.

'exitWriteMutex' is largely the same thing, complete with an
unprotectHeaderReadWriteArea/protectHeaderReadWriteArea wrapping a
increment/decrement to _theca->writerCount (depending on whether we
are entering or exiting the mutex respectively).

For 'doLockCache' and 'doUnlockCache', the caller must hold the write
mutex. The comments say as much. The header and read/write are
unprotected, the cache is locked/unlocked, and then the header and
read/write area are protected again. 



--

There are many more secrets of the CompositeCache to be discovered,
and for that we have to see how it's being used in the wild,
probably!! For instance, many of CompositeCache's features -- reader
count and writer count -- are outward facing, and don't affect the
internals of the cache beyond their values being
manipulated. Similarly, there are getter/setter functions for fields
like _next.. that point to subsequent caches in a chain of caches (the
comments spread throughout the implementation suggest that there is a
chain of 'supercaches' -- the top level caches that potentially
contain nested caches). But nothing further! All manipulation of
those fields is done by users of the SH_CompositeCache class.

So, let's begin to look at CacheMap. Its member variables:

SH_CompositeCacheImpl* _cc;					/* current cache */

/* See other _writeHash fields below. Put U_64 at the top so the debug
 * extensions can more easily mirror the shape.
 */
 U_64 _writeHashStartTime;
 OMRSharedClassConfig* _sharedClassConfig;

SH_CompositeCacheImpl* _ccHead;				/* head of supercache list */
SH_CompositeCacheImpl* _cacheletHead;		/* head of all known cachelets */
SH_CompositeCacheImpl* _ccCacheletHead;		/* head of cachelet list for current cache */
SH_CompositeCacheImpl* _cacheletTail;		/* tail of all known cachelets */
SH_CompositeCacheImpl* _prevCClastCachelet;	/* Reference to the last allocated cachelet in the last supercache */
//	SH_ClasspathManager* _cpm;
//	SH_TimestampManager* _tsm;
//	SH_ROMClassManager* _rcm;
//	SH_ScopeManager* _scm;
SH_CompiledMethodManager* _cmm;
//	SH_ByteDataManager* _bdm;
SH_AttachedDataManager* _adm;
OMRPortLibrary* _portlib;
omrthread_monitor_t _refreshMutex;
bool _cacheCorruptReported;
U_64* _runtimeFlags;
const char* _cacheName;
const char* _cacheDir;
UDATA _localCrashCntr;

UDATA _writeHashAverageTimeMicros;
UDATA _writeHashMaxWaitMicros;
UDATA _writeHashSavedMaxWaitMicros;
UDATA _writeHashContendedResetHash;
/* Also see U_64 _writeHashStartTime above */

UDATA _verboseFlags;
UDATA _bytesRead;
U_32 _actualSize;
UDATA _cacheletCntr;
J9Pool* _ccPool;
uintptr_t  _minimumAccessedShrCacheMetadata;
uintptr_t _maximumAccessedShrCacheMetadata;
bool _metadataReleased;

/* True iff (*_runtimeFlags & J9SHR_RUNTIMEFLAG_ENABLE_NESTED). Set in startup().
* This flag is a misnomer. It indicates the cache is growable (chained), which also
* implies it contains cachelets. However, the cache may contain cachelets even
* if this flag is not set.
* NOT equivalent to SH_Manager::_isRunningNested.
*/
bool _runningNested;

/* True iff we allow growing the cache via chained supercaches. Set in startup().
* _runningNested requests the growing capability, but _growEnabled controls the
* support for it.
* Currently always false, because cache growing is unstable.
* Internal: Requires cachelets.
*/
bool _growEnabled;

/* For growable caches, the cache can only be serialized once, because serialization "corrupts"
* the original cache and renders it unusable. e.g. We fix up offsets in AOT methods.
* This flag indicates whether the cache has already been serialized.
* Access to this is not currently synchronized.
*/
bool _isSerialized;

bool _isAssertEnabled; /* flag to turn on/off assertion before acquiring local mutex */

SH_Managers * _managers;

We see from the top of the list that there are a number of manager
classes. They appear to decide how certain items are stored within the
cache, and encapsulate information and methods describing how the
items are to be written and read. Of course, we also have a composite
cache implementation object, _cc.

Let's take a look at how 'findCompiledMethod' works. It takes a
J9ROMMethod, the VM current thread, and parameters called
dataStart, codeStart, codeSize, and forceReplace.

--

Most of the work are delegated to the SH_Managers, whichever are
appropriate for the desired data. So we should probably refer to them!

The ROMClassResourceManager, once SH_CacheMap->storeCompiledMethod
gets it successfully, is passed to the cache map's
storeROMClassResource method. In storeROMClassResource, we start by
emitting some trace messages, and calling
permitAccessToResource. Which is just a getter that returns the field
_accessPermitted from localRRM, the SH_ROMClassResourceManager object
storeCompiledMethod passed to storeROMClassResource. If access is not
permitted, we return an error, emit a trace msg, etc.

Otherwise, we enter the write mutex on _ccHead, which is at the head
of the composite cache chain (making _ccHead a composite cache). If we
can't get the write mutex, once again, we return with a resource store
error.

Then we call runEntryPointChecks. According to the comments, it should
be run before any find/store operation made to the cache. With either
the read or write mutex held, if other threads are active. As an
overview.. it checks these things:

1) Is the head corrupt? If yes, exit with error.

2) Is the address out of the bounds of the cache? If yes, exit with error.

3) Is the cache running in read-only mode? If not, do we have the
write mutex? If we have the write mutex, check for a crash. If there
was a crash, fail and exit. If we don't have the write mutex.. keep going.

Next, refresh the hash tables! Do this by calling
refreshHashtables. Per the comments, it "update[s] hashtables with new
data that might have appeared in the cache." The refresh mutex is held
throughout its execution, which makes it thread safe!! We don't need
to hold the write mutex if we have the refresh mutex. I should
specify, refreshHashtables enters the refresh mutex, not its caller.

Once the refresh mutex is held, we call readCacheUpdates. From it, we
call readCache. readCache expects to hold either the read or refresh
mutex. readCache walks the cache items using the nextEntry function in
a do/while loop. It retrieves each entry as a ShcItem*. For those
non-NULL items, it finds the suitable Manager type, belonging to the
CompositeCache. It does this by calling getAndStartManagerForType.
Each manager needs to be initialized.. this is all that is meant by
'start'. They don't run on dedicated threads or anything like that.

Anyway, this returns an SH_Manager*, which in actuality points to one
of the manager subclass objects. We call storeNew on that manager
object, with the item and a SH_CompositeCacheImpl* (named cache) as
parameters. From the call site in readCacheUpdates, this is none other
than _cc, the cache map's current cache. Which actually does the
action of storing the item in the *manager's* hashtable, the field
_hashTable inside SH_Manager, the manager abstract class. A resource
can be stored inside the cache *before* it is registered in the
manager of the corresponding type.

Before we return to discussing readCache, an interlude: what does it
mean for a cache item to be stale?? We never bothered answering that
above.. shame on us! SH_CacheMap::isStale calls
SH_CompositeCacheImpl::stale, which is little more than a wrapper
around the macro CCITEMSTALE. It's defined like this:

#define CCITEMSTALE(ih) (J9SHR_READMEM((ih)->itemLen) & 0x1)

So, we're checking to see if a single bit is set! This doesn't answer
what staleness means exactly.. or at all, depending on your
definition. For now, it suffices to know that an item is stale if it's
been invalidated in some way. Looking to the implementation of
storeNew in ROMClassResourceManager, we see that stale entries found
within the manager hash table are deleted. So it's a fair assumption
that, as the adjective 'stale' implies, the items are no longer good,
and can be discarded unhesitantly.

The do/while loop continues iterating until either some read fails, or
there are no items left in the iteration, or the cache is found to be
corrupt. The iteration is over the cache, by the way.. the passed in
SH_CompositeCacheImpl* argument cache. Not the CacheMap's own
_cc. Once finished, check if cache has become corrupt, and report that
fact if it is. Then call cache->doneReadUpdates. And we are done.

doneReadUpdates receives the currentThread and updates, the number of
cache entries that were updated. If there was more than one update,
and the new count of updates exceeds _oldUpdateCount, we update
_oldUpdateCount now that we're finished. Then we call
_debugData->processUpdates(currentThread, this). If we are protecting
the cache area that contains ROM class (ie. the segment area, which is
flagged by the boolean _doSegmentProtect), we call
this->notifyPagesRead(). And from there we're done.

SH_CompositeCacheImpl::notifyPagesRead tells the cache that a series
of pages are being read. This executes only if the cache is unlocked,
and if memory protection on the cache has been enabled. The logic
calls setRegionPermissions after calculating whether the start pointer
is greater or less than the end pointer, which decides the direction
(forward or backward). These checks are present to perform page
rounding on the start and end pointers correctly. This is really just
a wrapper on setRegionPermissions with some particulars in place for
how the cache is laid out.

Question for Hang: why is locking the cache distinct from memory
protecting regions of the cache?? Further, what purpose does memory
protection serve that is different from those of the write and release
mutexes? Don't all these do the same thing, really? Does memory
protection also protect a region from the thread that holds one of the
mutexes?

Once we return from runEntryPointChecks.. we are back in
storeROMClassResource. If we force a replacement, by the way, the RRM
marks the item as stale! We then call addROMClassResourceToCache.
initBlockData is called in the _ccHead. Which sets the info in the
ShcItem* item appropriately. Then getCacheAreaForDataType is
called. It's a SH_CompositeCacheImpl*. If the cache isn't nested
in some other cache, it's just _ccHead! Then we check to see that
the address of the ROM resource is in the range of _cc's
getBaseAddress() and _cc->getCacheLastEffectiveAddress(). If not,
it's in a different superclass, and we must fail.

Otherwise, we do a switch on the resourceType. For
TYPE_COMPILED_METHOD, we call allocateAOT on the CompositeCache.
Otherwise, we call allocateJIT for TYPE_ATTACHED_DATA (which usually
pertains to JIT hints, and JIT related info in general). All other
types are allocated with allocateBlock.

These allocations fail if the pointer they return, which is always
stored in itemInCache, regardless of type, is NULL. We do the usual
error dance and return with J9SHR_RESOURCE_STORE_FULL.

If we succeed, it's the resourceDescriptor that encapsulates knowledge
of how to write the item of the specific data type to the cache.  It
does this by calling 'writeDataToCache', with the itemInCache (not
actually the item, but a pointer to where it will live) and the
romAddress. As we saw below, romAddress forms the key into the
hashtable of the RRM, which indexes all the ROMClassResource's stored
within the cache. In particular (speaking to our interests on this
project, currently), CompiledMethodManager's writeDataToCache wraps
the info into a CompiledMethodWrapper, a struct containing fields for
the codeLength, dataLength, and the romMethodOffset (the romAddress -
the value of the CompiledMethodWrapper*, an address in the cache --
this is an SRP). We call localRRM->storeNew after writeDataToCache
returns. Again, this merely updates the localRRM's hash table to
contain the location within the cache under the key, the address of
the ROMClassResource in question.

Then commitUpdate is called on the cacheAreaForAllocate (again, this
is usually just _ccHead). We have not documented 'commitUpdate'
previously. According to comments heading up 'commitUpdate's
definition, the purpose of 'commitUpdate' is to notify other JVMs of
the cache update. It does this mostly by relying on
'commitUpdateHelper'. Specifically, it updates various cache header
counters.

'commitUpdate' is quite involved, and the expose here will conclude
our discussion of storeROMClassResource, since it's the last thing to
be called!

First, 'startCriticalUpdate' is called by 'commitUpdateHelper'. This
removes memory protection from the header. It unprotects the header of
the read/write area, and increments the crash counter. Then it
returns. By 'crashing', the cache becomes corrupted. The cache will
crash if the critical update is not 'stopped' by decrementing the
crash counter again.

That's all 'startCriticalUpdate' does. We return to where we left off
in 'commitUpdateHelper'. First, the composite cache CRC is marked as
invalid because the cache is about to change. Based on whether the
read/write section or class segment sections were updated, we
increment the readWriteSRP (resp., segmentSRP) value. Or both,
if both were modified. How do we know they were modified? The values
of the uints _storedSegmentUsedBytes and _storedReadWriteUsedBytes
tell us so. If they are 0, they act as false in a boolean context
(like the condition of an if statement).

Metadata, AOT code, and JIT code can all go into the cache entry
section, so updateSRP is decremented similarly. Then we change
_oldUpdateCount to a newly calculated value stored at
_theca->updateCountPtr. Surrounding all this, on the read/write and
metadata areas, are booleans that determine whether we use memory
protection. Earlier we unprotect these regions so we could manipulate
them, yes? But now that we've finished successfully, without a crash,
we restore protections. This is what the nested calls to
'notifyPagesCommitted' do. It works much like 'notifyPagesRead', which
we documented earlier. 'notifyPagesRead' is called at the conclusion
of 'notifyPagesCommitted', in fact, but before that, once the
applicable regions are calculated according to page size roundings as
before, _oscache->syncUpdates is called. This is what actually
synchronizes the updates to disk, depending on the semantics of the
underlying OSCache. syncUpdates is a pure virtual function in OSCache
that must be overloaded by a concrete subclass, in other
words. SH_OSCachemmap::updateSync will call j9mmap_msymc, the memory
map sync primitive, to do its work, for instance.

This brings us back to where 'notifyPagesCommitted' was possibly
called within 'commitUpdateHelper'. We call 'endCriticalUpdate' next.
It calls 'protectHeaderReadWriteArea' again, and decrements the
crash counter, undoing the work of 'startCriticalUpdate'.

From 'commitUpdateHelper' we go on to call 'updateMetadataSegment'
after updating _totalStoredBytes and _storedMetadataUsedBytes. This
mostly updates _metadataSegmentPtr, if there's an address stored
in it. Otherwise the method returns.

And that's it for 'commitUpdateHelper'. It returns to 'commitUpdate',
where it does some stuff if nested caches are enabled, but otherwise
finishes.

Ok, now to findCompiledMethod (in shrinit.cpp).

It dispatches to CacheMap's findCompiledMethod. Which, in turn, calls
out to findROMClassResource. It checks that RRM (the
SH_ROMClassResourceManager* instance) permits access.. by calling
permitAccessToResource. If useReadMutex is specified, it calls
enterReadMutex, and fails if enterReadMutex does. Then it calls
runEntryPointChecks, which checks that the cache isn't locked, isn't
read-only, we hold the right mutexes, all the manager hash tables are
up to date, etc.

Then we generate a keyfrom the resourceDescriptor. resourceDescriptor
is of type SH_ROMClassResourceManager::SH_ResourceDescriptor*.
generateKey is defined as a pure virtual function there! So it's
expected to be covered by some inheriting class. The answer to this is
that several manager classes derive from SH_ROMClassResourceManager,
and these subclasses have internal classes subclassing
SH_ResourceDescriptor. SH_CompiledMethodResourceDescriptor in
SH_CompiledMethodManager is one example. If you look to its
generateKey method, it returns the address of the method as the key,
and does nothing else.

Still, it's weird that the ROMClassResourceDescriptor and
ROMClassResourceManager are passed as two separate arguments to
findROMClassResource! Very bizarre indeed. This is just a wrapper
around rrmTableLookup.

'findROMClass' is also pretty illustrative, enough though we are not
looking to load classes of any kind just yet. We started by checking
that we have ROMClassManager, and that the head of _ccHead isn't
running read only. Assuming all it good, we get the read mutex of
_ccHead, and call 'runEntryPointChecks'.

This brings us to another detour into SH_CompositeCacheImpl, this
time, into the 'testAndSetWriteHash' method. We wondered previously
about what the 'write hash' was for. From the method description:

Test whether a class is being loaded by another JVM.  If not, update
to indicate this JVM will load it.

Should be called if an attempt to find a class in the cache has failed.
If the hash field in the cache field is free (0), it is set, indicating that the JVM is going to load the class.
If the hash field has already been set, then this means that another JVM is loading the class.
If it is the same class, 1 is returned which indicates that it would be wise to wait. Otherwise, 0 is returned.

A hash value, hashValue, is passed in, that it is calculated from the
name of the class (from 'computeHashForUTF8'). The writeHash value in
the cache is called _theca->writeHash. The hash value has some bitwise
component pertaining to the write hash, which is captured by &&'ing it
with the value WRITEHASH_MASK. If _theca->writeHash is 0, we call
'setWriteHash'. Otherwise, if hashValue & WRITEHASH_MASK == cacheValue
& WRITEHASH_MASK, we shift _theca->writeHash right by WRITEHASH_SHIFT
bytes, indicating the ID of the JVM that is 'in the
cache'. _theca->writeHash is really an indicator of two things, then:
the ID of the JVM modifying the cache, and the 'true' write hash in
the bits preceding it, of which there are WRITEHASH_SHIFT.

If the VM ID doesn't match _commonCCInfo->vmID, return 1, or true,
indicating that yes, the caller VM should wait for the cache to be
released. The call of 'setWriteHash' is to indicate to other JVMs that
*this* JVM is loading the class.

'setWriteHash' is really just a compare and swap atomic operation on
_theca->writeHash, buffeted by calls to 'unprotectHeaderReadWriteArea'
and 'protectHeaderReadWriteArea'. That's all. Again, the new value is
recorded as such, which corroborates the account given above:

value = ((hashValue & WRITEHASH_MASK) | (_commonCCInfo->vmID << WRITEHASH_SHIFT));

The hash value, the 'true' hash of the write hash, is &&-masked by
WRITEHASH_MASK, and the rest is simply the VM ID shifted by the number
of bits composing WRITEHASH_MASK.

Back to 'findROMClass'! If 'testAndSetWriteHash' tells us we should
wait, whatever waiting means (we will find out shortly!), we do the
following. We calculate the amount of time we should wait in
milliseconds. We use some internal stats (_writeHashAverageTimeMicros
and _writeHashMaxWaitMicros) to determine the value of waitMillis.
Then we get endTime and startTime, initializing them to
j9time_usec_clock().

If _writeHashMaxWaitMicros != 0, then while
_ccHead->checkUpdates(currentThread) returns false (indicating that
the cache was not updated since it was last read and synchronized), we
update endTime, and see if endTime - startTime >=
_writeHashMaxWaitMicros. If so, we break from the loop we've entered!
(the while(!_ccHead->checkUpdates(currentThread)) loop, not the
enclosing do loop).

If not, we modify waitMillis accordingly and wait waitMillis
milliseconds by calling omrthread_sleep(waitMillis). There are some
rules governing how the waitMillis is increased until the max waiting
time is reached. I have no idea what inspired their design. See the
source for details.

Once we've escaped the while loop, we check again that
_ccHead->checkUpdates(currentThread) returns a positive amount. If so,
we capture the read mutex. If that doesn't succeed, we fail and
return. Then we call 'refreshHashtables', which we've already
covered. Like _ccHead->checkUpdates, 'refreshHashtables' returns the
number of items read (and written into hashtables in the process).

If that succeeds (ie. rv != -1), we call localRCM->locateROMClass.
From within the Manager's hash, which is why the calls to update were
necessary. Well, not. 'refreshHashtables' was necessary, which we know
from _ccHead->checkUpdates returning a positive value. If nothing had
been updated, 'refreshHashtables' wouldn't have been necessary.

Anyway, localRCM->locateROMClass returns an error code containing
LOCATE_ROMCLASS_RETURN_DO_TRY_WAIT (which can happen), we continue
with the enclosing do if the actualTimeMicros (the time waited) hasn't
exceeded the maximum wait time. Otherwise we set updates, the count
returned from _ccHead->checkUpdates(), to 0. If the return code wasn't
LOCATE_ROMCLASS_RETURN_DO_TRY_WAIT, and counter of wait periods is >
0, we call 'updateAverageWriteHashTime', so that the average of write
hash wait times so far is updated to reflect the last witnessed length
of time the write hash was waited on.

If updates == 0, it could be that the class being searched for doesn't
exist, at least, not to the cache.. If that's the case, set
_writeHashMaxWaitMicros to 0. Also record the startTime, and some
other stuff. At this point, retry is false, and so we leave the
do/while loop (which continues only if retry is true, naturally).

After checking the staleness of ClassPathEntryItem (and marking the
ROMClass as stale if so), if the ROM class was found, we call
_ccHead->tryResetWriteHash if are using the write hash. And we're
trying to reduce contention.. this is a runtime flag stored within the
cache, btw. If we hold the write hash, or if the write hash failed to
be set too many times, we reset it. This is all the action of
_ccHead->tryResetWriteHash.

Once finished, we 'updateAccessedShrCacheMetadataBounds', and
'updateBytesRead'. That's about it!

My question, currently: when do we call 'rollbackUpdate'? A: not from
anywhere within CacheMap, it would appear. So we won't document it
just yet.

'updateROMSegmentList' is mostly a wrapper around
'updateROMSegmentListForCache'. We iterate through the linked list of
caches (that are started!), calling 'updateROMSegmentListForCache' on
each.

'updateROMSegmentListForCache' does the following. It gets the current
ROM Segment. How you ask? By calling getCurrentROMSegment() on the
target cache. It returns _currentROMSegment. If it's NULL, it's
initialized via 'addNewROMImageSegment'. Mostly, it calls
'createNewSegment', and adds the resulting romSegment as a
J9AVLTreeNode to vm->classMemorySegments->avlTreeData, which is an
index of class memory segments, it looks like.

'createNewSegment' calls
vm->internalVMFunctions->allocateMemorySegmentListEntry to retrieve
the address of romSegment, a J9MemorySegment*, and its fields are
initialized. So, it does appear as ROM class segments are contained
*within* the shared cache! Although.. no. The ROM segment (called
romSegment) appears to simply be a descriptor record, that stores the
type, the size of the ROM class, the base address, its class loader,
etc. The base address it records belongs to the
cache. forCache->getBaseAddress(), if we're being specific. This is
none other CASTART(_theca). The beginning of the segment entry free
area.

'updateROMSegmentListForCache' then calls setCurrentROMSegment on the
cache. And it proceeds to walk the ROM class list within the cache,
using resident info from the ROM segment. Namely, the
currentSegment. If the ROM class size found is too great for what the
current ROM segment has recorded, create a new segment that reflects
the present size. This is the essence of the update.. the VM's view on
which ROM classes are contained with the cache can be out of step with
the state of the cache itself. As we've seen. Hence the need for
updates.

ROM classes are stored via 'commitROMClass'. That's what *adds* a ROM
class to the cache. Space for ROM classes is allocated via
'allocateROMClass'. Which is part of CacheMap, not CompositeCache,
somewhat confusingly. Starting with 'allocateROMClass' first.

'allocateROMClass' tries to allocate memory in the class debug
segment. Debug info pertaining to the class, I think. Then we call
'allocateROMClassOnly', and that's presumably where the interesting
stuff begins. It's assigned the returned address to pieces->romClass.
And there's a lot more logic dealing with the ClasspathManager,
marking previously stored versions of the class as stale if
found. There's plenty of trace messages recording what's going on
there. Either way though, we wind up calling 'allocateFromCache'.

'allocateFromCache' checks some settings, emits some trace messages,
and calls initBlockData and allocateWithSegment on the
localCacheAreaForAllocate, which is returned from
getCacheAreaForDataType. localCacheAreaForAllocate is indeed a
SH_CompositeCacheImpl*. We call
allocateCacheAreaForAllocate->allocateWithSegment.

allocateCacheAreaForAllocate->allocateWithSegment just wraps a call
onto SH_CompositeCacheImpl::allocate, with the allocate type being a
block. The segmentBuffer parameter passed into it is a BlockPtr*, so a
double pointer. allocate winds up writing the address of the region to
be written, to it, starting at line 2788 of CompositeCache.cpp in the
OpenJ9 source.

Back to commitROMClass. Now that the ROM class has been written into
the cache, we want to store it in one of two wrapper types:
ROMClassWrapper or ScopedROMClassWrapper. This is based on whether the
boolean useScope is true. useScope's value is true if neither
partitionInCache nor modContextInCache, both parameters to
'commitROMClass', are NULL values.

Even if useScope is false, srcw, the ScopedROMClassWrapper object
created in the body of 'commitROM', is initialized in several
parts. Its ClassPathEntryIndex, cpeIndex, is recorded, and timestamp
is set to 0. I have no clue how this ClassPathEntry business works,
currently. I'm now weighing to what degree it shapes the design of the
SCC, and whether I should investigate it further. Until I reach a
conclusion on that, I'm going to ignore it for the rest of this
miniature writeup of 'commitROMClass'.

I have some idea of what a modification context is, but not a
partition. It would be lovely if the writers of the SCC were sat down
and encouraged to use consistent language. I guess the onus for that
is now on us.

We call _rcm->storeNew to store the itemInCache. itemInCache is cast
to a ScopedROMClassManager*, regardless of whether useScope is
true. Then we call cacheAreaForAllocate->commitUpdate,
'updateROMSegmentList', we do some more stuff for the write
hash.. yeah. 

TODO: maybe answer this whole allocate JIT vs. allocate AOT thing??
Yes? What the limitations of the JIT and AOT sections really mean,
amid the structure of the composite cache? Also, getReaderCount!
Where does _metadataSegmentPtr point to????

